# Table of contents
1. [Introduction](#introduction)
  
    1. [How do we represent the meaning of a word?](#meaningOfWord)
    1. [How do we have usable meaning in a computer?](#meaningInComputer)

2. [Some paragraph](#paragraph1)
    1. [Sub paragraph](#subparagraph1)
3. [Another paragraph](#paragraph2)


# 1. Introduction <a name="introduction"></a>

## 1.1 定义

NLP = Natural language processing


## 1.2 How do we represent the meaning of a word?<a name="meaningOfWord"></a>

**meaning**

- 用一个词，词组表示的概念
- 一个人想用语音、符号等表达的想法
- 文章，艺术等作品中表达的思想


semantics_def.jpg

## 1.3 How do we have usable meaning in a computer?<a name="meaningInComputer"></a>

### 1.3.1 WordNet


#### Idea 💡 用一个包含同义词，上位词的词典

#### 例子 🌰

WordNet.jpg

#### 缺点 ❎

- 忽略词之间的细微差别
 
  - 'proficient' 不完全等同于 'good'

- 缺少单词的新含义,不可能持续更新
- 需要人工创造，调整
- 不能计算单词相似度
  
#### 优点 ✅

## 1.3 one-hot vector

#### Idea 💡 在传统NLP里，把单词作为离散符合：用one-hot vector表示

**Def** 用只有一位为1，其余都是0的，长度为 `|V|` 的向量表示。(`|V|` = 词汇量
）

#### 例子 🌰

```
motel = [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]
hotel = [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]
```

#### 缺点 ❎

- 不能表达词之间的相似性。
  -   任意两个one-hot 向量都垂直
-   向量维度很大，非常sparse

#### Solution ✅

- 用 WordNet 中的同义词来获得相似性（但不可行，比如WordNet中不可能收录所有同义词）  
- **Instead : learn to encode similarity in the vectors themselves**


## 1.4 Distributional semantics
#### Idea 💡 根据上下文推测单词含义
#### Definition

- **Distributional semantics** := A word’s meaning is given by the words that frequently appear close-by

#### 例子 🌰

banking_meaning.jpg

- **context** = When a word w appears in a text, its ***context*** is the set of words that appear nearby (within a fixed-size window).

- If you can explain what context it's correct to use a certain word, versus in what context would be the wrong world to use. => understand the meaning of the word 

## 1.5 统计语言模型
ref https://mp.weixin.qq.com/s/zDneR1BU6xvt8cndEF4_Xw
统计语言模型是用来计算一个句子的概率的概率模型，它通常基于一个语料库来构建。那什么叫做一个句子的概率呢？假设  表示由个词  按顺序构成的一个句子，则  的联合概率为：

被称为语言模型，即用来计算这个句子概率的模型。利用Bayes公式，上式可以被链式地分解为：

其中的条件概率  就是语言模型的参数，若这些参数已经全部算得，那么给定一个句子，就可以很快地计算出相应地概率了。

看起来好像很简单，是吧？但是，具体实现起来还是有点麻烦。例如，先来看看模型参数的个数。刚才是考虑一个给定的长度为T的句子，就需要计算T个参数。不防假设语料库对应词典的大小（即词汇量）为，那么，如果考虑长度为的任意句子，理论上就有种可能，而每种可能都要计算个参数，总共就需要计算  个参数。当然，这里只是简单估算，并没有考虑重复参数，但这个量级还是有点吓人。此外，这些概率计算好后，还得保存下来，因此，存储这些信息也需要很大的内存开销。

此外，这些参数如何计算呢？常见的方法有n-gram模型、决策树、最大熵模型、最大熵马尔可夫模型、条件随机场、神经网络等方法。本文只讨论n-gram模型和神经网络两种方法。



# 2. Word Embedding 词嵌入


## 2.2 Word vector
建立从单词到向量空间（维度<< `|V|` 词汇量）的一个映射（关系），使得每个单词都对应一个稠密的向量，并且与它上下文中的单词的向量相似。
这个向量被称为词向量（word vectors)，词嵌入（word embedding)或者 word representations

#### 例子 🌰

banking_vector.jpg

进入计算，学习词向量的算法前，我们看一个词向量的可视化。（为了方便，下图把9维词向量投射到二维空间）

word_vector_visualization.jpg
word_vector_visualization2.jpg
## 2.3 Word2vec 
### 2.3.1 Overview
Word2vec (Mikolov et al. 2013) 是一个学习词向量的框架。

#### Idea 💡
- 我们有大量文本（a large corpus of text）
- 词汇表中的每个单词都由一个向量表示
- 遍历文本中的每个位置 t，都有对应的中心词（center word） c 和上下文（ context (“outside”)） o
- 通过单词 c 和 o 的词向量的相似度，来计算给定c出现o的概率（反之亦然）
- 通过不断调整词向量，最大化这个概率

下图为`P(w_{t+j}|w_t)`的计算过程，window size = 2， 中心词 = into

into_example.jpg

### 2.3.2 Details

Word2vec 包含：
- **两个学习词向量的模型** 
  
  - CBOW 通过上下文预测中心词
  - Skip-gram 通过中心词预测上下文
 
 cbow&skipgram.png.jpg
 
 由图可见，两个模型都包含三层：**输入层**、**投影层**和**输出层**
 
- **两个训练方法** 降低模型学习过程中的运算量
  
  - negative sampling 负采样
  - hierarchical softmax

### 2.3.2 Continuous Bag of Words Model (CBOW)

#### 结构
1. simple CBOW
simple_cbow.jpg

2. Multi-Word Context Model
multi_cbow.jpg

cbow.jpg

**目标** 对每一个单词 w，我们要学习2个向量
- v (输入向量）当 w 在上下文中；
- u（输出向量）当 w 是中心词 

**例子** 🌰
有下面的句子：

*"The cat jumped over the puddle."*

希望通过上下文 {"The", "cat", "over", "the", "puddle"} 来预测/生成中心词 "jumped"

***In math*** 希望 通过{"The", "cat", ’over", "the’, "puddle"}的词向量得到向量 = "jumped"的词向量

**Notation for CBOW Model**
- `w_i` : Word i from vocabulary V
- `V \in R^{n\times |V|}: Input word matrix
- `v_i` : i-th column of V , the input vector representation of word `w_i`
- `U \in R^{|V|\times n}: Output word matrix
- `u_i` : i-th row of U , the output vector representation of word `w_i`

我们用 `w_{c-m}, ..., w_{c-1}, w_{c+1},...,w_{c+m}` 代表上下文， `w_c`代表中心词。假设已有所有词的one-hot vector表示。
Input:x, output:y

1. 生成上下文中的单词的 one-hot 向量: `x^{c-m}, ..., x^{c-1}, x^{c+1},...,x^{c+m} \in R^{|V|}`.
2. 得到上下文的词向量：`v_{c-m} = Vx^{c-m}, ..., v_{c-1} = Vx^{c-1}, v_{c+1} = Vx^{c+1},...,v_{c+m} = Vx^{c+m} \in R^{n}`
3. 计算平均值 `\hat{v} = \frac{v_{c-m} +...+ v_{c-1} + v_{c+1} + ...+v_{c+m}}{2m} \in R^{|V|}`

  1. 有的地方直接求和，不影响

4. 生成分数向量 `z = U\hat(v) \in R^{|V|}`

  - 这样，由于相似向量的点积大，所以为了得到高分，会向相似单词的词向量靠近。

6. 通过softmax函数把分数转化为概率 `\hat{y} = \text{softmax}(z) \in R^{|V|}`
7. 我们希望生成的概率`\hat{y}\in R^{|V|}` 与真实 `y\in R^{|V|}` 相匹配，注意这里`y` 也是one-hot。

#### 目标函数 Obejective function 
For each position 𝑡 = 1, ... , 𝑇, predict center word `w_c`, given the context words within a window of fixed size m `w_{c-m}, ..., w_{c-1}, w_{c+1},...,w_{c+m}`. 

Data likelihood:

``
L(\theta) = \Pi_{c = 1} ^T  P(w_c|w_{c-m}, ..., w_{c-1}, w_{c+1},...,w_{c+m})
``

``
L(\theta) = \Pi_{t = 1} ^T \Pi_{-m\leq j \leq m, j\neq 0} P(w_c|w_{c-m}, ..., w_{c-1}, w_{c+1},...,w_{c+m})
``


- `\theta` 包含所有需要优化的参数（i.e. 两个词向量矩阵）

The objective function `J_{\theta}` is the (average) negative log likelihood:

``
J = - \frac{1}{T}\log L(\theta) 
``

**Question: How to calculate `P(o|c)`

- `𝑃(𝑢_{𝑝𝑟𝑜𝑏𝑙𝑒𝑚𝑠} | 𝑣_{𝑖𝑛𝑡𝑜}) short for P(𝑝𝑟𝑜𝑏𝑙𝑒𝑚𝑠 | 𝑖𝑛𝑡𝑜 ; 𝑢_{𝑝𝑟𝑜𝑏𝑙𝑒𝑚𝑠}, 𝑣_{𝑖𝑛𝑡𝑜},\theta)


prob.jpg

#### Gradiant descent and stochastic gradient descent


### 2.3.3 Skip-gram

skip_gram

## 2.4 基于Hierarchical Softmax的模型

## 2.5  基于Negative Sampling的模型
neg_sampling1.jpg
neg_sampling2.jpg
neg_sampling3.jpg
neg_sampling4.jpg
具体例子


