# Table of contents
1. [Introduction](#introduction)
  
    1. [How do we represent the meaning of a word?](#meaningOfWord)
    1. [How do we have usable meaning in a computer?](#meaningInComputer)

2. [Some paragraph](#paragraph1)
    1. [Sub paragraph](#subparagraph1)
3. [Another paragraph](#paragraph2)


# 1. Introduction <a name="introduction"></a>

## 1.1 å®šä¹‰

NLP = Natural language processing


## 1.2 How do we represent the meaning of a word?<a name="meaningOfWord"></a>

**meaning**

- ç”¨ä¸€ä¸ªè¯ï¼Œè¯ç»„è¡¨ç¤ºçš„æ¦‚å¿µ
- ä¸€ä¸ªäººæƒ³ç”¨è¯­éŸ³ã€ç¬¦å·ç­‰è¡¨è¾¾çš„æƒ³æ³•
- æ–‡ç« ï¼Œè‰ºæœ¯ç­‰ä½œå“ä¸­è¡¨è¾¾çš„æ€æƒ³


semantics_def.jpg

## 1.3 How do we have usable meaning in a computer?<a name="meaningInComputer"></a>

### 1.3.1 WordNet


#### Idea ğŸ’¡ ç”¨ä¸€ä¸ªåŒ…å«åŒä¹‰è¯ï¼Œä¸Šä½è¯çš„è¯å…¸

#### ä¾‹å­ ğŸŒ°

WordNet.jpg

#### ç¼ºç‚¹ â

- å¿½ç•¥è¯ä¹‹é—´çš„ç»†å¾®å·®åˆ«
 
  - 'proficient' ä¸å®Œå…¨ç­‰åŒäº 'good'

- ç¼ºå°‘å•è¯çš„æ–°å«ä¹‰,ä¸å¯èƒ½æŒç»­æ›´æ–°
- éœ€è¦äººå·¥åˆ›é€ ï¼Œè°ƒæ•´
- ä¸èƒ½è®¡ç®—å•è¯ç›¸ä¼¼åº¦

    -  

#### ä¼˜ç‚¹ âœ…

## 1.3 one-hot vector

#### Idea ğŸ’¡ åœ¨ä¼ ç»ŸNLPé‡Œï¼ŒæŠŠå•è¯ä½œä¸ºç¦»æ•£ç¬¦åˆï¼šç”¨one-hot vectorè¡¨ç¤º

**Def** ç”¨åªæœ‰ä¸€ä½ä¸º1ï¼Œå…¶ä½™éƒ½æ˜¯0çš„ï¼Œé•¿åº¦ä¸º `|V|` çš„å‘é‡è¡¨ç¤ºã€‚(`|V|` = è¯æ±‡é‡
ï¼‰

#### ä¾‹å­ ğŸŒ°

```
motel = [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]
hotel = [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]
```

#### ç¼ºç‚¹ â

- ä¸èƒ½è¡¨è¾¾è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
  -   ä»»æ„ä¸¤ä¸ªone-hot å‘é‡éƒ½å‚ç›´
-   å‘é‡ç»´åº¦å¾ˆå¤§ï¼Œéå¸¸sparse

#### Solution âœ…

- ç”¨ WordNet ä¸­çš„åŒä¹‰è¯æ¥è·å¾—ç›¸ä¼¼æ€§ï¼ˆä½†ä¸å¯è¡Œï¼Œæ¯”å¦‚WordNetä¸­ä¸å¯èƒ½æ”¶å½•æ‰€æœ‰åŒä¹‰è¯ï¼‰  
- **Instead : learn to encode similarity in the vectors themselves**

# 2. Word Embedding è¯åµŒå…¥

## 2.1 Distributional semantics
#### Idea ğŸ’¡ æ ¹æ®ä¸Šä¸‹æ–‡æ¨æµ‹å•è¯å«ä¹‰
#### Definition

- **Distributional semantics** := A wordâ€™s meaning is given by the words that frequently appear close-by

#### ä¾‹å­ ğŸŒ°

banking_meaning.jpg

- **context** = When a word w appears in a text, its ***context*** is the set of words that appear nearby (within a fixed-size window).

- If you can explain what context it's correct to use a certain word, versus in what context would be the wrong world to use. => understand the meaning of the word 

## 2.2 Word vector
å»ºç«‹ä»å•è¯åˆ°å‘é‡ç©ºé—´ï¼ˆç»´åº¦<< `|V|` è¯æ±‡é‡ï¼‰çš„ä¸€ä¸ªæ˜ å°„ï¼ˆå…³ç³»ï¼‰ï¼Œä½¿å¾—æ¯ä¸ªå•è¯éƒ½å¯¹åº”ä¸€ä¸ªç¨ å¯†çš„å‘é‡ï¼Œå¹¶ä¸”ä¸å®ƒä¸Šä¸‹æ–‡ä¸­çš„å•è¯çš„å‘é‡ç›¸ä¼¼ã€‚
è¿™ä¸ªå‘é‡è¢«ç§°ä¸ºè¯å‘é‡ï¼ˆword vectors)ï¼Œè¯åµŒå…¥ï¼ˆword embedding)æˆ–è€… word representations

#### ä¾‹å­ ğŸŒ°

banking_vector.jpg

è¿›å…¥è®¡ç®—ï¼Œå­¦ä¹ è¯å‘é‡çš„ç®—æ³•å‰ï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸ªè¯å‘é‡çš„å¯è§†åŒ–ã€‚ï¼ˆä¸ºäº†æ–¹ä¾¿ï¼Œä¸‹å›¾æŠŠ9ç»´è¯å‘é‡æŠ•å°„åˆ°äºŒç»´ç©ºé—´ï¼‰

word_vector_visualization.jpg
word_vector_visualization2.jpg
## 2.3 Word2vec 
### 2.3.1 Overview
Word2vec (Mikolov et al. 2013) æ˜¯ä¸€ä¸ªå­¦ä¹ è¯å‘é‡çš„æ¡†æ¶ã€‚

#### Idea ğŸ’¡
- æˆ‘ä»¬æœ‰å¤§é‡æ–‡æœ¬ï¼ˆa large corpus of textï¼‰
- è¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªå•è¯éƒ½ç”±ä¸€ä¸ªå‘é‡è¡¨ç¤º
- éå†æ–‡æœ¬ä¸­çš„æ¯ä¸ªä½ç½® tï¼Œéƒ½æœ‰å¯¹åº”çš„ä¸­å¿ƒè¯ï¼ˆcenter wordï¼‰ c å’Œä¸Šä¸‹æ–‡ï¼ˆ context (â€œoutsideâ€)ï¼‰ o
- é€šè¿‡å•è¯ c å’Œ o çš„è¯å‘é‡çš„ç›¸ä¼¼åº¦ï¼Œæ¥è®¡ç®—ç»™å®šcå‡ºç°oçš„æ¦‚ç‡ï¼ˆåä¹‹äº¦ç„¶ï¼‰
- é€šè¿‡ä¸æ–­è°ƒæ•´è¯å‘é‡ï¼Œæœ€å¤§åŒ–è¿™ä¸ªæ¦‚ç‡

ä¸‹å›¾ä¸º`P(w_{t+j}|w_t)`çš„è®¡ç®—è¿‡ç¨‹ï¼Œwindow size = 2ï¼Œ ä¸­å¿ƒè¯ = into

into_example.jpg

### 2.3.2 Details

Word2vec åŒ…å«ï¼š
- **ä¸¤ä¸ªå­¦ä¹ è¯å‘é‡çš„æ¨¡å‹** 
  
  - CBOW é€šè¿‡ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯
  - Skip-gram é€šè¿‡ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡
 
 cbow&skipgram.png.jpg
 
 ç”±å›¾å¯è§ï¼Œä¸¤ä¸ªæ¨¡å‹éƒ½åŒ…å«ä¸‰å±‚ï¼š**è¾“å…¥å±‚**ã€**æŠ•å½±å±‚**å’Œ**è¾“å‡ºå±‚**
 
- **ä¸¤ä¸ªè®­ç»ƒæ–¹æ³•** é™ä½æ¨¡å‹å­¦ä¹ è¿‡ç¨‹ä¸­çš„è¿ç®—é‡
  
  - negative sampling è´Ÿé‡‡æ ·
  - hierarchical softmax

### 2.3.2 Continuous Bag of Words Model (CBOW)

#### ç»“æ„
cbow.jpg

**ç›®æ ‡** å¯¹æ¯ä¸€ä¸ªå•è¯ wï¼Œæˆ‘ä»¬è¦å­¦ä¹ 2ä¸ªå‘é‡
- v (è¾“å…¥å‘é‡ï¼‰å½“ w åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼›
- uï¼ˆè¾“å‡ºå‘é‡ï¼‰å½“ w æ˜¯ä¸­å¿ƒè¯ 

**ä¾‹å­** ğŸŒ°
æœ‰ä¸‹é¢çš„å¥å­ï¼š

*"The cat jumped over the puddle."*

å¸Œæœ›é€šè¿‡ä¸Šä¸‹æ–‡ {"The", "cat", "over", "the", "puddle"} æ¥é¢„æµ‹/ç”Ÿæˆä¸­å¿ƒè¯ "jumped"

***In math*** å¸Œæœ› é€šè¿‡{"The", "cat", â€™over", "theâ€™, "puddle"}çš„è¯å‘é‡å¾—åˆ°å‘é‡ = "jumped"çš„è¯å‘é‡

**Notation for CBOW Model**
- `w_i` : Word i from vocabulary V
- `V \in R^{n\times |V|}: Input word matrix
- `v_i` : i-th column of V , the input vector representation of word `w_i`
- `U \in R^{|V|\times n}: Output word matrix
- `u_i` : i-th row of U , the output vector representation of word `w_i`

æˆ‘ä»¬ç”¨ `w_{c-m}, ..., w_{c-1}, w_{c+1},...,w_{c+m}` ä»£è¡¨ä¸Šä¸‹æ–‡ï¼Œ `w_c`ä»£è¡¨ä¸­å¿ƒè¯ã€‚å‡è®¾å·²æœ‰æ‰€æœ‰è¯çš„one-hot vectorè¡¨ç¤ºã€‚

1. ç”Ÿæˆä¸Šä¸‹æ–‡ä¸­çš„å•è¯çš„ one-hot å‘é‡: `x^{c-m}, ..., x^{c-1}, x^{c+1},...,x^{c+m} \in R^{|V|}`.
2. å¾—åˆ°ä¸Šä¸‹æ–‡çš„è¯å‘é‡ï¼š`v_{c-m} = Vx^{c-m}, ..., v_{c-1} = Vx^{c-1}, v_{c+1} = Vx^{c+1},...,v_{c+m} = Vx^{c+m} \in R^{n}`

We get our embedded word vectors for the context (vcâˆ’m = Vx(câˆ’m), vcâˆ’m+1 = Vx(câˆ’m+1), . . ., vc+m = Vx(c+m) âˆˆ Rn)
Average these vectors to get vË† = vcâˆ’m+vcâˆ’m+1+...+vc+m âˆˆ Rn 2m
Generate a score vector z = UvË† âˆˆ R|V|. As the dot product of similar vectors is higher, it will push similar words close to each other in order to achieve a high score.
Turn the scores into probabilities yË† = softmax(z) âˆˆ R|V|.
We desire our probabilities generated, yË† âˆˆ R|V|, to match the true
probabilities, y âˆˆ R|V|, which also happens to be the one hot vector of the actual word.

### 2.3.3 Skip-gram




