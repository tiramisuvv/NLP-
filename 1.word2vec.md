# Table of contents
1. [Introduction](#introduction)
  
    1. [How do we represent the meaning of a word?](#meaningOfWord)
    1. [How do we have usable meaning in a computer?](#meaningInComputer)

2. [Some paragraph](#paragraph1)
    1. [Sub paragraph](#subparagraph1)
3. [Another paragraph](#paragraph2)


# 1. Introduction <a name="introduction"></a>

## 1.1 定义

NLP = Natural language processing


## 1.2 How do we represent the meaning of a word?<a name="meaningOfWord"></a>

**meaning**

- 用一个词，词组表示的概念
- 一个人想用语音、符号等表达的想法
- 文章，艺术等作品中表达的思想


semantics_def.jpg

## 1.3 How do we have usable meaning in a computer?<a name="meaningInComputer"></a>

### 1.3.1 WordNet


#### Idea 💡 用一个包含同义词，上位词的词典

#### 例子 🌰

WordNet.jpg

#### 缺点 ❎

- 忽略词之间的细微差别
 
  - 'proficient' 不完全等同于 'good'

- 缺少单词的新含义,不可能持续更新
- 需要人工创造，调整
- 不能计算单词相似度

    -  

#### 优点 ✅

## 1.3 one-hot vector

#### Idea 💡 在传统NLP里，把单词作为离散符合：用one-hot vector表示

**Def** 用只有一位为1，其余都是0的，长度为 `|V|` 的向量表示。(`|V|` = 词汇量
）

#### 例子 🌰

```
motel = [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]
hotel = [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]
```

#### 缺点 ❎

- 不能表达词之间的相似性。
  -   任意两个one-hot 向量都垂直
-   向量维度很大，非常sparse

#### Solution ✅

- 用 WordNet 中的同义词来获得相似性；
  - 不完整  
- **Instead : learn to encode similarity in the vectors themselves**

# 2. Word Embedding 词嵌入
## 2.1 Distributional semantics
**Def** **Distributional semantics: A word’s meaning is given by the words that frequently appear close-by


**Idea** 💡 用一个字典来记录词之间的关系（近义词，反义词）
    h<sub>&theta;</sub>(x) = &theta;<sub>o</sub> x + &theta;<sub>1</sub>x

**例子** 🌰

**缺点** ❎

1. 更新慢
2. 忽略词之间的差别

**优点** ✅
