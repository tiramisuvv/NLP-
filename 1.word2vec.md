# Table of contents
1. [Introduction](#introduction)
  
    1. [How do we represent the meaning of a word?](#meaningOfWord)
    1. [How do we have usable meaning in a computer?](#meaningInComputer)

2. [Some paragraph](#paragraph1)
    1. [Sub paragraph](#subparagraph1)
3. [Another paragraph](#paragraph2)


# 1. Introduction <a name="introduction"></a>

## 1.1 定义

NLP = Natural language processing


## 1.2 How do we represent the meaning of a word?<a name="meaningOfWord"></a>

**meaning**

- 用一个词，词组表示的概念
- 一个人想用语音、符号等表达的想法
- 文章，艺术等作品中表达的思想


semantics_def.jpg

## 1.3 How do we have usable meaning in a computer?<a name="meaningInComputer"></a>

### 1.3.1 WordNet


#### Idea 💡 用一个包含同义词，上位词的词典

#### 例子 🌰

WordNet.jpg

#### 缺点 ❎

- 忽略词之间的细微差别
 
  - 'proficient' 不完全等同于 'good'

- 缺少单词的新含义,不可能持续更新
- 需要人工创造，调整
- 不能计算单词相似度

    -  

#### 优点 ✅

## 1.3 one-hot vector

#### Idea 💡 在传统NLP里，把单词作为离散符合：用one-hot vector表示

**Def** 用只有一位为1，其余都是0的，长度为 `|V|` 的向量表示。(`|V|` = 词汇量
）

#### 例子 🌰

```
motel = [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]
hotel = [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]
```

#### 缺点 ❎

- 不能表达词之间的相似性。
  -   任意两个one-hot 向量都垂直
-   向量维度很大，非常sparse

#### Solution ✅

- 用 WordNet 中的同义词来获得相似性（但不可行，比如WordNet中不可能收录所有同义词）  
- **Instead : learn to encode similarity in the vectors themselves**

# 2. Word Embedding 词嵌入

## 2.1 Distributional semantics
#### Idea 💡 根据上下文推测单词含义
#### Definition

- **Distributional semantics** := A word’s meaning is given by the words that frequently appear close-by

#### 例子 🌰

banking_meaning.jpg

- **context** = When a word w appears in a text, its ***context*** is the set of words that appear nearby (within a fixed-size window).

- If you can explain what context it's correct to use a certain word, versus in what context would be the wrong world to use. => understand the meaning of the word 

## 2.2 Word vector
建立从单词到向量空间（维度<< `|V|` 词汇量）的一个映射（关系），使得每个单词都对应一个稠密的向量，并且与它上下文中的单词的向量相似。
这个向量被称为词向量（word vectors)，词嵌入（word embedding)或者 word representations

#### 例子 🌰

banking_vector.jpg

进入计算，学习词向量的算法前，我们看一个词向量的可视化。（为了方便，下图把9维词向量投射到二维空间）

word_vector_visualization.jpg
word_vector_visualization2.jpg
## 2.3 Word2vec 
### 2.3.1 Overview
Word2vec (Mikolov et al. 2013) 是一个学习词向量的框架。

#### Idea 💡
- 我们有大量文本（a large corpus of text）
- 词汇表中的每个单词都由一个向量表示
- 遍历文本中的每个位置 t，都有对应的中心词（center word） c 和上下文（ context (“outside”)） o
- 通过单词 c 和 o 的词向量的相似度，来计算给定c出现o的概率（反之亦然）
- 通过不断调整词向量，最大化这个概率

下图为`P(w_{t+j}|w_t)`的计算过程，window size = 2， 中心词 = into

into_example.jpg

### 2.3.2 Details

Word2vec 包含：
- **两个学习词向量的模型** 
  
  - CBOW 通过上下文预测中心词
  - Skip-gram 通过中心词预测上下文
 
 cbow&skipgram.png.jpg
 
 由图可见，两个模型都包含三层：**输入层**、**投影层**和**输出层**
 
- **两个训练方法** 降低模型学习过程中的运算量
  
  - negative sampling 负采样
  - hierarchical softmax

### 2.3.2 Continuous Bag of Words Model (CBOW)

#### 结构
cbow.jpg

**目标** 对每一个单词 w，我们要学习2个向量
- v (输入向量）当 w 在上下文中；
- u（输出向量）当 w 是中心词 

**例子** 🌰
有下面的句子：

*"The cat jumped over the puddle."*

希望通过上下文 {"The", "cat", "over", "the", "puddle"} 来预测/生成中心词 "jumped"

***In math*** 希望 通过{"The", "cat", ’over", "the’, "puddle"}的词向量得到向量 = "jumped"的词向量

**Notation for CBOW Model**
- `w_i` : Word i from vocabulary V
- `V \in R^{n\times |V|}: Input word matrix
- `v_i` : i-th column of V , the input vector representation of word `w_i`
- `U \in R^{|V|\times n}: Output word matrix
- `u_i` : i-th row of U , the output vector representation of word `w_i`

我们用 `w_{c-m}, ..., w_{c-1}, w_{c+1},...,w_{c+m}` 代表上下文， `w_c`代表中心词。假设已有所有词的one-hot vector表示。

1. 生成上下文中的单词的 one-hot 向量: `x^{c-m}, ..., x^{c-1}, x^{c+1},...,x^{c+m} \in R^{|V|}`.
2. 得到上下文的词向量：`v_{c-m} = Vx^{c-m}, ..., v_{c-1} = Vx^{c-1}, v_{c+1} = Vx^{c+1},...,v_{c+m} = Vx^{c+m} \in R^{n}`

We get our embedded word vectors for the context (vc−m = Vx(c−m), vc−m+1 = Vx(c−m+1), . . ., vc+m = Vx(c+m) ∈ Rn)
Average these vectors to get vˆ = vc−m+vc−m+1+...+vc+m ∈ Rn 2m
Generate a score vector z = Uvˆ ∈ R|V|. As the dot product of similar vectors is higher, it will push similar words close to each other in order to achieve a high score.
Turn the scores into probabilities yˆ = softmax(z) ∈ R|V|.
We desire our probabilities generated, yˆ ∈ R|V|, to match the true
probabilities, y ∈ R|V|, which also happens to be the one hot vector of the actual word.

### 2.3.3 Skip-gram




